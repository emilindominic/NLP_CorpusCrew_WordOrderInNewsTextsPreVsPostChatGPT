{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f2c74ef",
   "metadata": {},
   "source": [
    "# Text Pre-Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09d22042",
   "metadata": {},
   "source": [
    "The requirement of Milestone 1 is to pre-process the text data for our Word-Order analysis task. We chose 3 languages from the Leipzig Corpora Collection provided in the task description for the following years, classifying it into 2 periods - Pre and Post ChatGPT:\n",
    "1. English: 2018-2020(PRE); 2023-2024(POST)\n",
    "2. German: 2019-Nov,2022(PRE); Dec,2022-2024(POST)\n",
    "3. Russian: 2013, 2019, 2020, Jan 2022-Nov 2022(PRE); 2024(POST)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00102123",
   "metadata": {},
   "source": [
    "### Import dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "27675c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f4c96c2",
   "metadata": {},
   "source": [
    "### Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afc95bab",
   "metadata": {},
   "source": [
    "We define a function to clean individual sentences by removing unwanted characters and normalizing spaces. Along with that sentences which are too short were removed from the dataset. \n",
    "All the sentences from the sentences text files are merged with the date from the metadata text files using the files named \"inv_so\" which contains all the sentence IDs to their corresponding source IDs. \n",
    "In the end, for each language a dataframe is made combining data from the three text files for the columns \"Language\", \"Year\", \"Period\", \"Date\" and \"Sentence\". These are stored in the form of TSV files. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "869dd0e1",
   "metadata": {},
   "source": [
    "Functions for cleaning the corpus and merging them into a dataframe based on languages and another one with all the languages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5ab3c99b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_sentence(sentence):\n",
    "    # Remove URLs\n",
    "    sentence = re.sub(r'http\\S+|www\\S+|https\\S+', '', sentence, flags=re.MULTILINE)\n",
    "    \n",
    "    # Remove HTML tags\n",
    "    sentence = re.sub(r'<.*?>', '', sentence)\n",
    "    \n",
    "    # Remove quotation marks \n",
    "    sentence = sentence.strip('\"')\n",
    "    \n",
    "    # Normalize spaces\n",
    "    sentence = re.sub(r\"\\s+\", \" \", sentence).strip()\n",
    "\n",
    "    return sentence\n",
    "\n",
    "def process_corpus(base_path: Path, language: str):\n",
    "    \"\"\"\n",
    "    Reads Leipzig-style corpus files (-sentences, -sources, -inv_so),\n",
    "    merges them, cleans text, and adds Pre/Post-ChatGPT period.\n",
    "    \"\"\"\n",
    "    base = str(base_path).replace(\"-sentences.txt\", \"\")\n",
    "    \n",
    "    # Load core files\n",
    "    sentences = pd.read_csv(f\"{base}-sentences.txt\", sep=\"\\t\", header=None,\n",
    "                            names=[\"sentence_id\", \"sentence\"],\n",
    "                            quoting=csv.QUOTE_NONE, encoding=\"utf-8-sig\",\n",
    "                            on_bad_lines=\"skip\", engine=\"python\")\n",
    "    \n",
    "    try:\n",
    "        sources = pd.read_csv(f\"{base}-sources.txt\", sep=\"\\t\", header=None,\n",
    "                              names=[\"source_id\", \"url\", \"date\"],\n",
    "                              quoting=csv.QUOTE_NONE, encoding=\"utf-8-sig\",\n",
    "                              on_bad_lines=\"skip\", engine=\"python\")\n",
    "        \n",
    "        inv_so = pd.read_csv(f\"{base}-inv_so.txt\", sep=\"\\t\", header=None,\n",
    "                             names=[\"source_id\", \"sentence_id\"],\n",
    "                             quoting=csv.QUOTE_NONE, encoding=\"utf-8-sig\",\n",
    "                             on_bad_lines=\"skip\", engine=\"python\")\n",
    "        \n",
    "        # Merge to attach dates\n",
    "        df = inv_so.merge(sources, on=\"source_id\", how=\"left\").merge(sentences, on=\"sentence_id\", how=\"left\")\n",
    "        df[\"date\"] = pd.to_datetime(df[\"date\"], errors=\"coerce\")\n",
    "        cutoff = pd.Timestamp(\"2022-11-30\")\n",
    "        df[\"period\"] = df[\"date\"].apply(lambda d: \"Pre-ChatGPT\" if pd.notnull(d) and d <= cutoff else \"Post-ChatGPT\")\n",
    "        df[\"year\"] = df[\"date\"].dt.year\n",
    "    \n",
    "    except FileNotFoundError:\n",
    "        # Fall back: no metadata available\n",
    "        print(f\"⚠️ No date metadata for {language}. Using filename year only.\")\n",
    "        year_match = re.search(r\"(\\d{4})\", base_path.name)\n",
    "        year = int(year_match.group(1)) if year_match else None\n",
    "        period = \"Pre-ChatGPT\" if year < 2023 else \"Post-ChatGPT\"\n",
    "        df = pd.read_csv(base_path, sep=\"\\t\", header=None, names=[\"sentence_id\", \"sentence\"],\n",
    "                         quoting=csv.QUOTE_NONE, encoding=\"utf-8-sig\", on_bad_lines=\"skip\", engine=\"python\")\n",
    "        df[\"year\"] = year\n",
    "        df[\"period\"] = period\n",
    "        df[\"date\"] = None\n",
    "    \n",
    "    total_before = len(df)  # Track length before cleaning\n",
    "    # Add language and clean\n",
    "    df[\"language\"] = language\n",
    "    df = df.dropna(subset=[\"sentence\"])\n",
    "    df[\"sentence\"] = df[\"sentence\"].astype(str).str.strip().apply(clean_sentence)\n",
    "    df = df[df[\"sentence\"].str.len() > 30]\n",
    "    total_after = len(df)  # Tracking length after cleaning\n",
    "    print(f\"Processed {language}: {total_before} sentences before cleaning, {total_after} after cleaning.\")\n",
    "    \n",
    "    return df[[\"language\", \"year\", \"period\", \"date\", \"sentence\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "803c21e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Processing language: English\n",
      "Processing eng_news_2020_100K-sentences.txt...\n",
      "Processed English: 100000 sentences before cleaning, 96901 after cleaning.\n",
      "Processing eng_news_2024_100K-sentences.txt...\n",
      "Processed English: 100000 sentences before cleaning, 97382 after cleaning.\n",
      "\n",
      " Processing language: German\n",
      "Processing deu_news_2024_100K-sentences.txt...\n",
      "Processed German: 100000 sentences before cleaning, 98165 after cleaning.\n",
      "\n",
      " Processing language: Russian\n",
      "Processing rus_news_2020_100K-sentences.txt...\n",
      "Processed Russian: 100000 sentences before cleaning, 96429 after cleaning.\n",
      "Processing rus_news_2024_100K-sentences.txt...\n",
      "Processed Russian: 100000 sentences before cleaning, 97646 after cleaning.\n",
      "\n",
      " All corpora processed and saved in 'data/clean/'\n"
     ]
    }
   ],
   "source": [
    "# --- Auto-discover and process all datasets ---\n",
    "base_dir = Path(\"data/raw\")\n",
    "output_dir = Path(\"data/clean\")\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "all_dfs = []\n",
    "\n",
    "for lang_dir in base_dir.iterdir():\n",
    "    if lang_dir.is_dir():  # e.g. english, german, russian\n",
    "        language = lang_dir.name.capitalize()\n",
    "        print(f\"\\n Processing language: {language}\")\n",
    "        \n",
    "        for file in lang_dir.rglob(\"*-sentences.txt\"):\n",
    "            print(f\"Processing {file.name}...\")\n",
    "            \n",
    "            df = process_corpus(file, language)\n",
    "            \n",
    "            # Output file name based on year or fallback\n",
    "            df[\"year\"] = df[\"year\"].astype(\"Int64\")\n",
    "            year = int(df[\"year\"].mode()[0]) if not df[\"year\"].dropna().empty else \"unknown\"\n",
    "            out_path = output_dir / f\"{language.lower()}_{year}.csv\"\n",
    "            df.to_csv(out_path.with_suffix(\".tsv\"), sep=\"\\t\", index=False)\n",
    "            all_dfs.append(df)\n",
    "\n",
    "# Combine everything\n",
    "merged = pd.concat(all_dfs, ignore_index=True)\n",
    "merged.to_csv(output_dir / \"all_languages_clean.tsv\", index=False, sep=\"\\t\")\n",
    "\n",
    "print(\"\\n All corpora processed and saved in 'data/clean/'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c6d491",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
